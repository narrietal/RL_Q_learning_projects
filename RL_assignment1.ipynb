{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwc0grTpBRVF"
      },
      "source": [
        "# Reinforcement learning - Assignment 1\n",
        "\n",
        "Course: Data Mining for Networks\n",
        "\n",
        "Author: Nicolas Arrieta Larraza\n",
        "\n",
        "Date: 17-02-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H6wXs5rbo1r"
      },
      "source": [
        "## GYM exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvZJJE3Okc3k"
      },
      "source": [
        "### Initializating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdLs1n8skqyN"
      },
      "source": [
        "#### Importing libraries and necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emdC0MgXb8PE",
        "outputId": "3a432e04-76d9-4a86-b82c-26ee2e4c6ec1"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "env = gym.make(\"Taxi-v3\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTLltNFnkzC1"
      },
      "source": [
        "#### Setting up state, action and Q-table matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUo-US_jcQCc",
        "outputId": "e83b6326-e4bc-4146-a9e1-2ad55a8474da"
      },
      "source": [
        "state_space = env.observation_space.n\n",
        "action_space = env.action_space.n\n",
        "qtable = np.zeros((state_space,action_space))\n",
        "print(\"Q-table initialized with zeros and with dimension:\",qtable.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q-table initialized with zeros and with dimension: (500, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ULfpbAk9bh"
      },
      "source": [
        "### Training algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq4f61ZdlA52"
      },
      "source": [
        "Let's first define the hyperparameters of the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj4Du1dTc17N"
      },
      "source": [
        "epsilon = 1.0 #Max greed\n",
        "epsilon_min = 0.005 #Min greed\n",
        "epsilon_decay = 0.99994 #Rate of epsilon decay after each episode\n",
        "episodes = 50000 #Number of games\n",
        "max_steps = 100 #Max steps per episide\n",
        "lr = 0.65 #Learning rate \n",
        "gamma = 0.65 #Discount factor (1 for long-term rewards, 0 for immediate rewards) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs0FQiWfp3iw"
      },
      "source": [
        "Definiton of complete Q-learning training algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vafV0fWdg9S"
      },
      "source": [
        "def q_learning_algorithm(epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma):\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    #Reset the game parameters\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "\n",
        "      #Take best action in Q-table (exploitation)\n",
        "      if np.random.uniform(0,1) > epsilon:\n",
        "        action = np.argmax(qtable[state,:])\n",
        "      #Take random action (exploration)\n",
        "      else:\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "      #Take a step\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      #add up score\n",
        "      score += reward\n",
        "\n",
        "      #Update Q-table with new Q value\n",
        "      qtable[state,action] = (1-lr)*qtable[state,action] + lr*(reward+gamma*np.max(qtable[next_state,:]))\n",
        "\n",
        "      #Update state\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    #Reducing epsilon each episode (exploration-exploitation trade-off)\n",
        "    if epsilon >= epsilon_min:\n",
        "      epsilon += epsilon_decay\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j5to_67LXPZ"
      },
      "source": [
        "Let's compute the training algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoNcrfBXtT-P"
      },
      "source": [
        "q_learning_algorithm(epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZsyWL6XqioD"
      },
      "source": [
        "### Testing algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rCsrZxMLBID"
      },
      "source": [
        "Let's compare the performance of the Q-learning algorithm vs a simple brute-force algoritm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sX_o8AZI65u"
      },
      "source": [
        "def test_q_learning(episodes, max_steps):\n",
        "\n",
        "  total_epochs, total_penalties = 0, 0\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    #Reset the game parameters\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    epochs, penalties = 0, 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "\n",
        "      action = np.argmax(qtable[state,:])\n",
        "\n",
        "      #Take a step\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      #add up score\n",
        "      score += reward\n",
        "\n",
        "      #Update state\n",
        "      state = next_state\n",
        "\n",
        "      epochs+=1\n",
        "\n",
        "      if reward < 0:\n",
        "        penalties+=1\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "    \n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "  \n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDapyydJYkF"
      },
      "source": [
        "def test_simple_algorithm(episodes, max_steps):\n",
        "\n",
        "  total_epochs, total_penalties = 0, 0\n",
        "  \n",
        "  for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    epochs, penalties = 0, 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "      #Take random action\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "      #Take a step\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      #add up score\n",
        "      score += reward\n",
        "\n",
        "      #Update state\n",
        "      state = next_state\n",
        "\n",
        "      epochs+=1\n",
        "\n",
        "      if reward < 0:\n",
        "        penalties+=1\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "        \n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "  \n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuFU1q0TNff9"
      },
      "source": [
        "Definition of testing parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGWxVrURJ3yi"
      },
      "source": [
        "test_episodes = 500\n",
        "test_max_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPrl60NFK4Mr",
        "outputId": "d4139220-634b-47d7-d0cc-4ada92528710"
      },
      "source": [
        "test_q_learning(test_episodes, test_max_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 13.044\n",
            "Average penalties per episode: 12.044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWppHVkFkIeV",
        "outputId": "fb8204ae-fcc6-464d-9514-c5808d32e05e"
      },
      "source": [
        "test_simple_algorithm(test_episodes, test_max_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 500 episodes:\n",
            "Average timesteps per episode: 99.762\n",
            "Average penalties per episode: 99.748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJTh255vQmqI"
      },
      "source": [
        "In the light of the results one can observe the efectivity of Q-learning against a brute-force algorithm:\n",
        "\n",
        "\n",
        "*   The brute force algorithm takes **~86 more steps**, meaning an increase of **664.81%**\n",
        "*   The brute force algorithm commits **~88 more penalties** meaning an increase of **728.2%**\n",
        "\n"
      ]
    }
  ]
}