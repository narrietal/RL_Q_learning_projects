{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI_RL_assignment_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5L7bTCMaOjYt"},"source":["# AI Reinforcement Learning Assignment 1\n","\n","Author: Nicolas Arrieta Larraza\n","\n","Date: 25/03/2021"]},{"cell_type":"markdown","metadata":{"id":"e6JX2CwaO4iH"},"source":["## Importing"]},{"cell_type":"code","metadata":{"id":"QgIVRyS2p_pb","executionInfo":{"status":"ok","timestamp":1617262243697,"user_tz":-120,"elapsed":1574,"user":{"displayName":"Thuany Stuart","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXgSTl5HN6O3hiZnt8j2NRV780mzhwxZmLLjppxo=s64","userId":"11603451811245298910"}}},"source":["import numpy as np\n","import random\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16w3NOldO67f"},"source":["## Defining parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vp9c_ugXqNMj","executionInfo":{"status":"ok","timestamp":1617262244381,"user_tz":-120,"elapsed":2232,"user":{"displayName":"Thuany Stuart","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDXgSTl5HN6O3hiZnt8j2NRV780mzhwxZmLLjppxo=s64","userId":"11603451811245298910"}},"outputId":"5b8ac395-9d09-4da9-c83d-aac0b43b7416"},"source":["states = 6 #Number of states\n","actions = 6 #Number of actions\n","\n","# Reward matrix\n","R = np.array([[0,-0.001,0,0,0,0],\n","                   [-1,0,-0.001,0,0,0],\n","                   [0,-0.001,0,-0.001,0,0],\n","                   [0,0,-0.001,0,-0.001,0],\n","                   [0,0,0,-0.001,0,1],\n","                   [0,0,0,0,-0.001,0]])\n","\n","#Q table initialized with zeros\n","Q_table = np.zeros([states,actions])\n","print(\"Q-table initialized with zeros and with dimension:\",Q_table.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Q-table initialized with zeros and with dimension: (6, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzhcweejr507"},"source":["epsilon = 1.0 #Max greed\n","epsilon_min = 0.005 #Min greed\n","epsilon_decay = -0.005 #Rate of epsilon decay after each episode\n","episodes = 10000 #Number of games\n","max_steps = 100 #Max steps per episode\n","lr = 0.65 #Learning rate \n","gamma = 0.65 #Discount factor (1 for long-term rewards, 0 for immediate rewards) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JPszM8WPt_o"},"source":["## Q-learning algorithm"]},{"cell_type":"code","metadata":{"id":"ScJ-u5LZsK_Q"},"source":["def q_learning_algorithm(epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma):\n","\n","  for episode in range(episodes):\n","    score = 0\n","    state = random.randint(0, 5)\n","\n","    for _ in range(max_steps):\n","\n","      invalid_move = True\n","      #Take best action in Q-table (exploitation)\n","      if np.random.uniform(0,1) > epsilon:\n","        action = np.argmax(Q_table[state,:])\n","      #Take random action (exploration)\n","      else:\n","        while invalid_move:\n","          rnd_action = random.randint(0,5)\n","          if R[state,:][rnd_action]!=0: \n","            invalid_move = False\n","            action = rnd_action\n","\n","      #Take a step\n","      next_state = action\n","      reward = R[state,action]\n","\n","      #add up score\n","      score += reward\n","\n","      #Update Q-table with new Q value\n","      Q_table[state,action] = Q_table[state,action] + lr*(reward + gamma*np.max(Q_table[next_state,:]) - Q_table[state,action])\n","\n","      if state == 5: break\n","\n","      #Update state\n","      state = next_state\n","\n","    #Reducing epsilon each episode (exploration-exploitation trade-off)\n","    if epsilon >= epsilon_min: epsilon += epsilon_decay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5jg2F2u4u25"},"source":["q_learning_algorithm(epsilon, epsilon_min, epsilon_decay, episodes, max_steps, lr, gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7SpmErrPzL1"},"source":["After the training the Q-table is updated as following:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyu2oE4bG1C0","executionInfo":{"status":"ok","timestamp":1616673032571,"user_tz":-60,"elapsed":848,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"8542515f-4bcb-4297-8cee-dca41a02019e"},"source":["Q_table"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.        ,  0.30655369,  0.        ,  0.        ,  0.        ,\n","         0.        ],\n","       [-0.8007401 ,  0.        ,  0.47315952,  0.        ,  0.        ,\n","         0.        ],\n","       [ 0.        ,  0.30655369,  0.        ,  0.72947619,  0.        ,\n","         0.        ],\n","       [ 0.        ,  0.        ,  0.47315952,  0.        ,  1.12380952,\n","         0.        ],\n","       [ 0.        ,  0.        ,  0.        ,  0.72947619,  0.        ,\n","         1.73047619],\n","       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.12380952,\n","         0.        ]])"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"Z9-4gBqPP5Fi"},"source":["## Testing implementation"]},{"cell_type":"code","metadata":{"id":"PkLc_dfE5bfI"},"source":["def test_q_learning(episodes, max_steps):\n","\n","  total_epochs, total_penalties = 0, 0\n","\n","  for episode in range(episodes):\n","    #Reset the game parameters\n","    score = 0\n","    state = random.randint(0, 5)\n","    epochs, penalties = 0, 0\n","\n","    for _ in range(max_steps):\n","\n","      action = np.argmax(Q_table[state,:])\n","\n","      #Take a step\n","      next_state = action\n","      reward =  R[state,action]\n","\n","      #add up score\n","      score += reward\n","\n","      epochs+=1\n","\n","      if reward < 0:\n","        penalties+=1\n","\n","      if state == 5: break\n","\n","      #Update state\n","      state = next_state\n","    \n","    total_penalties += penalties\n","    total_epochs += epochs\n","  \n","  print(f\"Results after {episodes} episodes:\")\n","  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","  print(f\"Average penalties per episode: {total_penalties / episodes}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCxOiqI06PXt","executionInfo":{"status":"ok","timestamp":1616673024487,"user_tz":-60,"elapsed":795,"user":{"displayName":"Nicolás Arrieta Larraza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjapwfbKe9Al_rwk8yEvjMYpFfs2-PBZuUSakFB=s64","userId":"06800015717016616953"}},"outputId":"25feb089-5768-4803-fca1-8b9fd277d00a"},"source":["test_episodes = 500\n","test_max_steps = 100\n","test_q_learning(test_episodes, test_max_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Results after 500 episodes:\n","Average timesteps per episode: 3.424\n","Average penalties per episode: 2.598\n"],"name":"stdout"}]}]}